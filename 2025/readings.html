<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/2025/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/2025/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(4)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(4) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(4) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list { display: block; } </style> <script src="/2025/assets/js/vendor/lunr.min.js"></script> <script src="/2025/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Readings | DS-GA.3001</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Readings" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Course Website" /> <meta property="og:description" content="Course Website" /> <link rel="canonical" href="https://embodied-learning-vision-course.github.io/2025/readings.html" /> <meta property="og:url" content="https://embodied-learning-vision-course.github.io/2025/readings.html" /> <meta property="og:site_name" content="DS-GA.3001" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Readings" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Course Website","headline":"Readings","url":"https://embodied-learning-vision-course.github.io/2025/readings.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/2025/" class="site-title lh-tight"> DS-GA.3001 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/2025/" class="nav-list-link">About</a></li><li class="nav-list-item"><a href="/2025/content.html" class="nav-list-link">Calendar</a></li><li class="nav-list-item"><a href="/2025/projects.html" class="nav-list-link">Projects</a></li><li class="nav-list-item"><a href="/2025/readings.html" class="nav-list-link">Readings</a></li><li class="nav-list-item"><a href="/2025/staff.html" class="nav-list-link">Staff</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search DS-GA.3001" aria-label="Search DS-GA.3001" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <p><strong>Module 0: Introduction</strong></p> <ul> <li>Turing (1950) <a href="https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf">Computing Machinery and Intelligence</a></li> <li>Pomerleau (1988) <a href="https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf">ALVINN: An Autonomous Land Vehicle in a Neural Network</a></li> <li>[Video] <a href="https://www.youtube.com/watch?v=2KMAAmkz9go">History Channel 1998 : Driverless Car Technology Overview at Carnegie Mellon University</a></li> <li>Smith &amp; Gasser (2005) <a href="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf">The Development of Embodied Cognition: Six Lessons from Babies</a></li> </ul> <p><strong>Module 1: Deep Learning for Structured Outputs</strong></p> <ul> <li>Suggested readings <ul> <li>LeCun (2006) <a href="https://www.cs.toronto.edu/~vnair/ciar/lecun1.pdf">A Tutorial on Energy-Based Learning</a></li> <li>Girshick et al. (2013) <a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li> <li>Long et al. (2014) <a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a></li> <li>Zheng et al. (2015) <a href="https://arxiv.org/abs/1502.03240">Conditional Random Fields as Recurrent Neural Networks</a></li> <li>Chen et al. (2016) <a href="https://arxiv.org/abs/1606.00915">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></li> <li>Kingma &amp; Dhariwal (2018) <a href="https://arxiv.org/abs/1807.03039">Glow: Generative Flow with Invertible 1x1 Convolutions</a></li> <li>Ho et al. (2020) <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></li> </ul> </li> <li>Additional readings <ul> <li>Carion et al. (2020) <a href="https://arxiv.org/pdf/2005.12872">End-to-End Object Detection with Transformers</a></li> <li>Kamath et al. (2021) <a href="https://arxiv.org/abs/2104.12763">MDETR â€“ Modulated Detection for End-to-End Multi-Modal Understanding</a></li> <li>Cheng et al. (2021) <a href="https://arxiv.org/abs/2107.06278">Per-Pixel Classification is Not All You Need for Semantic Segmentation</a></li> <li>Rombach et al. (2022) <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li> <li>Kirillov et al. (2023) <a href="https://arxiv.org/abs/2304.02643">Segment Anything</a></li> <li>Bai et al. (2023) <a href="https://arxiv.org/abs/2312.00785">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></li> <li>Chi et al. (2023) <a href="https://arxiv.org/abs/2303.04137">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</a></li> </ul> </li> </ul> <p><strong>Module 2: 3D Vision and Mapping</strong></p> <ul> <li>Suggested readings: <ul> <li>Fischer et al. (2015) <a href="https://arxiv.org/abs/1504.06852">FlowNet: Learning Optical Flow with Convolutional Networks</a></li> <li>Godard et al. (2016) <a href="https://arxiv.org/abs/1609.03677">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a></li> <li>Qi et al. (2016) <a href="https://arxiv.org/abs/1612.00593">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></li> <li>Tamar et al. (2016) <a href="https://arxiv.org/abs/1602.02867">Value Iteration Networks</a></li> <li>Parisotto et al. (2017) <a href="https://arxiv.org/abs/1702.08360">Neural Map: Structured Memory for Deep Reinforcement Learning</a></li> <li>Gupta et al. (2017) <a href="https://arxiv.org/abs/1702.03920">Cognitive Mapping and Planning for Visual Navigation</a></li> </ul> </li> <li>Additional readings: <ul> <li>Chaplot et al. (2020) <a href="https://arxiv.org/abs/2005.12256">Neural Topological SLAM for Visual Navigation</a></li> <li>Huang et al. (2022) <a href="https://arxiv.org/abs/2203.16194">FlowFormer: A Transformer Architecture for Optical Flow</a></li> <li>Wu et al. (2023) <a href="https://arxiv.org/abs/2301.01006">Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling</a></li> <li>Sun et al. (2023) <a href="https://arxiv.org/abs/2310.18887">Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes</a></li> <li>Yang et al. (2024) <a href="https://arxiv.org/abs/2401.10891">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</a></li> <li>Wang et al. (2025) <a href="https://arxiv.org/abs/2501.12387">Continuous 3D Perception Model with Persistent State</a></li> </ul> </li> </ul> <p><strong>Module 3: Self-Supervised Representation Learning and Object Discovery</strong></p> <ul> <li>Suggested readings: <ul> <li>Sermanet et al. (2017) <a href="https://arxiv.org/abs/1704.06888">Time-Contrastive Networks: Self-Supervised Learning from Video</a></li> <li>Van den Oord et al. (2018) <a href="https://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Coding</a></li> <li>Wu et al. (2018) <a href="https://arxiv.org/abs/1805.01978">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</a></li> <li>Chen et al. (2020) <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></li> <li>Grill et al. (2020) <a href="https://arxiv.org/abs/2006.07733">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</a></li> <li>He et al. (2021) <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></li> </ul> </li> <li>Additional readings: <ul> <li>Weinzaepfel et al. (2022) <a href="https://arxiv.org/abs/2211.10408">CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow</a></li> <li>Wang et al. (2022) <a href="https://arxiv.org/abs/2202.11539">Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut</a></li> <li>Seo et al. (2022) <a href="https://arxiv.org/abs/2206.14244">Masked World Models for Visual Control</a></li> <li>Venkataramanan et al. (2023) <a href="https://arxiv.org/abs/2310.08584">Is ImageNet Worth 1 Video? Learning Strong Image Encoders from 1 Long Unlabelled Video</a></li> <li>van Steenkiste et al. (2024) <a href="https://arxiv.org/abs/2411.05927">Moving Off-the-Grid: Scene-Grounded Video Representations</a></li> <li>Cui et al. (2024) <a href="https://arxiv.org/abs/2409.12192">DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</a></li> <li>Wang et al. (2024) <a href="https://arxiv.org/abs/2408.11208">PooDLe: Pooled and Dense Self-Supervised Learning from Naturalistic Videos</a></li> </ul> </li> </ul> <p><strong>Module 4: World Models and End-to-End Planning</strong></p> <ul> <li>Suggested readings: <ul> <li>Ross et al. (2011) <a href="https://arxiv.org/abs/1011.0686">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</a></li> <li>Kalchbrenner et al. (2016) <a href="https://arxiv.org/abs/1610.00527">Video Pixel Networks</a></li> <li>Ha and Schmidhuber (2018) <a href="https://arxiv.org/abs/1803.10122">World Models</a></li> <li>Haarnoja et al. (2018) <a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></li> <li>Srinivas et al. (2018) <a href="https://arxiv.org/abs/1804.00645">Universal Planning Networks</a></li> <li>Sukhbaatar et al. (2018) <a href="https://arxiv.org/abs/1703.05407">Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play</a></li> <li>Amos et al. (2018) <a href="https://arxiv.org/abs/1810.13400">Differentiable MPC for End-to-end Planning and Control</a></li> <li>Hafner et al. (2019) <a href="https://arxiv.org/abs/1912.01603">Dream to Control: Learning Behaviors by Latent Imagination</a></li> <li>Zeng et al. (2019) <a href="https://arxiv.org/abs/2101.06679">End-to-end Interpretable Neural Motion Planner</a></li> </ul> </li> <li>Additional readings: <ul> <li>Liang et al. (2020) <a href="https://arxiv.org/abs/2007.13732">Learning Lane Graph Representations for Motion Forecasting</a></li> <li>Casas et al. (2021) <a href="https://arxiv.org/abs/2101.06806">MP3: A Unified Model to Map, Perceive, Predict and Plan</a></li> <li>Chaplot et al. (2021) <a href="https://arxiv.org/abs/2112.01010">Differentiable Spatial Planning using Transformers</a></li> <li>Wu et al. (2022) <a href="https://arxiv.org/abs/2206.14176">DayDreamer: World Models for Physical Robot Learning</a></li> <li>Yu et al. (2022) <a href="https://arxiv.org/abs/2212.05199">MAGVIT: Masked Generative Video Transformer</a></li> <li>Hu et al. (2022) <a href="https://arxiv.org/abs/2212.10156">Planning-oriented Autonomous Driving</a></li> <li>Dinev et al. (2022) <a href="https://arxiv.org/abs/2209.01117">Differentiable Optimal Control via Differential Dynamic Programming</a></li> <li>Hafner et al. (2023) <a href="https://arxiv.org/abs/2301.04104">Mastering Diverse Domains through World Models</a></li> <li>Hansen et al. (2023) <a href="https://arxiv.org/abs/2310.16828">TD-MPC2: Scalable, Robust World Models for Continuous Control</a></li> <li>Hu et al. (2023) <a href="https://arxiv.org/abs/2309.17080">GAIA-1: A Generative World Model for Autonomous Driving</a></li> <li>Chi et al. (2023) <a href="https://arxiv.org/abs/2303.04137">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</a></li> <li>Zhang et al. (2024) <a href="https://arxiv.org/abs/2311.01017">Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion</a></li> <li>Casas et al. (2024) <a href="https://arxiv.org/abs/2406.04426">DeTra: A Unified Model for Object Detection and Trajectory Forecasting</a></li> <li>Bruce et al. (2024) <a href="https://arxiv.org/abs/2402.15391">Genie: Generative Interactive Environments</a></li> <li>Psenka et al. (2024) <a href="https://arxiv.org/abs/2312.11752">Learning a Diffusion Model Policy from Rewards via Q-Score Matching</a></li> </ul> </li> </ul> <p><strong>Module 5: Continual Learning and Meta-Learning</strong></p> <ul> <li>Suggested readings: <ul> <li>Marsland (2002) <a href="https://www.sciencedirect.com/science/article/pii/S0893608002000783">A Self-Organising Network that Grows when Required</a></li> <li>Kirkpatrick et al. (2016) <a href="https://arxiv.org/abs/1612.00796">Overcoming catastrophic forgetting in neural networks</a></li> <li>Rebuffi et al. (2016) <a href="https://arxiv.org/abs/1611.07725">iCaRL: Incremental Classifier and Representation Learning</a></li> <li>Yoon et al. (2017) <a href="https://arxiv.org/abs/1708.01547">Lifelong Learning with Dynamically Expandable Networks</a></li> <li>Nguyen et al. (2017) <a href="https://arxiv.org/abs/1710.10628">Variational Continual Learning</a></li> <li>Van de Ven et al. (2020) <a href="https://www.nature.com/articles/s41467-020-17866-2">Brain-Inspired Replay for Continual Learning with Artificial Neural Networks</a></li> <li>Fei-Fei &amp; Fergus (2006) <a href="http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf">One-Shot Learning of Object Categories</a></li> <li>Lake et al. (2011) <a href="https://cims.nyu.edu/~brenden/papers/LakeEtAl2011CogSci.pdf">One-Shot Learning of Simple Visual Concepts</a></li> <li>Snell et al. (2017) <a href="https://arxiv.org/abs/1703.05175">Prototypical Networks for Few-shot Learning</a></li> <li>Finn et al. (2017) <a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li> <li>James et al. (2018) <a href="https://arxiv.org/abs/1810.03237">Task-Embedded Control Networks for Few-Shot Imitation Learning</a></li> <li>Brown et al. (2020) <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li> <li>Chen et al. (2021) <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Meta-Baseline_Exploring_Simple_Meta-Learning_for_Few-Shot_Learning_ICCV_2021_paper.pdf">Exploring Simple Meta-Learning for Few-Shot Learning</a></li> </ul> </li> <li>Additional readings: <ul> <li>Javed &amp; White (2019) <a href="https://arxiv.org/abs/1905.12588">Meta-Learning Representations for Continual Learning</a></li> <li>Lake (2019) <a href="https://arxiv.org/abs/1906.05381">Compositional Generalization through Meta Sequence-to-Sequence Learning</a></li> <li>Dohare et al. (2021) <a href="https://arxiv.org/abs/2108.06325">Continual Backprop: Stochastic Gradient Descent with Persistent Randomness</a></li> <li>Wang et al. (2021) <a href="https://arxiv.org/abs/2112.08654">Learning to Prompt for Continual Learning</a></li> <li>Ren et al. (2021) <a href="https://arxiv.org/abs/2007.04546">Wandering Within a World: Online Contextualized Few-Shot Learning</a></li> <li>Alayrac et al. (2022) <a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a></li> <li>Song et al. (2022) <a href="https://arxiv.org/abs/2212.04088">LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</a></li> <li>Powers et al. (2023) <a href="https://arxiv.org/abs/2306.02413">Evaluating Continual Learning on a Home Robot</a></li> <li>Zhang et al. (2023) <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf">A Novel Visual Question Answering Continual Learning Setting</a></li> <li>Lee et al. (2023) <a href="https://arxiv.org/abs/2310.08204">STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment</a></li> <li>Majumder et al. (2023) <a href="https://arxiv.org/abs/2310.10134">CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</a></li> </ul> </li> </ul> <p><strong>Module 6: LLM Agents</strong></p> <ul> <li>Suggested readings: <ul> <li>Langley et al. (2009) <a href="https://www.sciencedirect.com/science/article/abs/pii/S1389041708000557">Cognitive architectures: Research issues and challenges</a></li> <li>Misra et al. (2017) <a href="https://arxiv.org/abs/1704.08795">Mapping Instructions and Visual Observations to Actions with Reinforcement Learning</a></li> <li>Andreson et al. (2018) <a href="https://arxiv.org/abs/1711.07280">Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments</a></li> <li>Andreas (2022) <a href="https://arxiv.org/abs/2212.01681">Language Models as Agent Models</a></li> <li>Sridhar et al. (2023) <a href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2023.1217093/full">Cognitive Neuroscience Perspective on Memory: Overview and Summary</a></li> </ul> </li> <li>Additional readings: <ul> <li>Anh et al. (2022) <a href="https://arxiv.org/abs/2204.01691">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li> <li>Sumers et al. (2023) <a href="https://arxiv.org/abs/2309.02427">Cognitive Architectures for Language Agents</a></li> <li>Schick et al. (2023) <a href="https://arxiv.org/pdf/2302.04761">Language Models Can Teach Themselves to Use Tools</a></li> <li>Rana et al. (2023) <a href="https://arxiv.org/abs/2307.06135">SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning</a></li> <li>Kim et al. (2024) <a href="https://arxiv.org/abs/2407.18550">ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments</a></li> <li>Li et al. (2024) <a href="https://arxiv.org/abs/2410.07166">Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</a></li> </ul> </li> </ul> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
