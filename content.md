---
title: Calendar
layout: page
---

Week 1 Jan 23
: **Lec**{: .label .label-green }[Introduction](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lectures/week01_intro.pdf)
  **Tut**{: .label .label-purple }[HPC tutorial](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lab/lab1_hpc.pdf)
: - Content
  - History of self-driving cars
  - Embodied learning
  - Suggested readings
    - Turing (1950) [Computing Machinery and Intelligence](https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf)
	- Pomerleau (1988) [ALVINN: An Autonomous Land Vehicle in a Neural Network](https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf)
	- [Video] [History Channel 1998 : Driverless Car Technology Overview at Carnegie Mellon University](https://www.youtube.com/watch?v=2KMAAmkz9go)
	- Smith & Gasser (2005) [The Development of Embodied Cognition: Six Lessons from Babies](https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf)

Week 2 Jan 30
: **Lec**{: .label .label-green }[Deep Learning for Structured Outputs](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lectures/week02_structured_learning.pdf)
  **Tut**{: .label .label-purple }[Simulator Tutorial](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lab/lab2_simulator.pdf)
: - Content
  - Object detection and segmentation
  - Graphical models
  - Energy-based models
  - Autoregressive models
  - Suggested readings
    - LeCun (2006) [A Tutorial on Energy-Based Learning](https://www.cs.toronto.edu/~vnair/ciar/lecun1.pdf)
    - Girshick et al. (2013) [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)
    - Long et al. (2014) [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038)
    - Zheng et al. (2015) [Conditional Random Fields as Recurrent Neural Networks](https://arxiv.org/abs/1502.03240)
    - Chen et al. (2016) [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)
    - Kingma & Dhariwal (2018) [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)
    - Ho et al. (2020) [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
  - Additional readings
    - Carion et al. (2020) [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872)
    - Kamath et al. (2021) [MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding](https://arxiv.org/abs/2104.12763)
    - Cheng et al. (2021) [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278)
    - Rombach et al. (2022) [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    - Kirillov et al. (2023) [Segment Anything](https://arxiv.org/abs/2304.02643)
    - Bai et al. (2023) [Sequential Modeling Enables Scalable Learning for Large Vision Models](https://arxiv.org/abs/2312.00785)
    - Chi et al. (2023) [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/abs/2303.04137)

Week 3 Feb 6
: **Lec**{: .label .label-green }[3D Vision, Mapping](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lectures/week03_3d_mapping.pdf)
  **Tut**{: .label .label-purple }[Video Learning Tutorial](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lab/lab3_video_learning.pdf)
: - Diffusion models
    - Probabilistic foundation
    - Applications in embodied learning
  - 3D network designs
    - Bird's eye view networks
    - Point cloud networks
  - Suggested readings:
    - Fischer et al. (2015) [FlowNet: Learning Optical Flow with Convolutional Networks](https://arxiv.org/abs/1504.06852)
    - Godard et al. (2016) [Unsupervised Monocular Depth Estimation with Left-Right Consistency](https://arxiv.org/abs/1609.03677)
    - Qi et al. (2016) [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)
    - Tamar et al. (2016) [Value Iteration Networks](https://arxiv.org/abs/1602.02867)
    - Parisotto et al. (2017) [Neural Map: Structured Memory for Deep Reinforcement Learning](https://arxiv.org/abs/1702.08360)
    - Gupta et al. (2017) [Cognitive Mapping and Planning for Visual Navigation](https://arxiv.org/abs/1702.03920)
  - Additional readings: 
    - Chaplot et al. (2020) [Neural Topological SLAM for Visual Navigation](https://arxiv.org/abs/2005.12256)
    - Huang et al. (2022) [FlowFormer: A Transformer Architecture for Optical Flow](https://arxiv.org/abs/2203.16194)
    - Wu et al. (2023) [Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling](https://arxiv.org/abs/2301.01006)
    - Sun et al. (2023) [Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes](https://arxiv.org/abs/2310.18887)
    - Yang et al. (2024) [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891)
    - Wang et al. (2025) [Continuous 3D Perception Model with Persistent State](https://arxiv.org/abs/2501.12387)

Week 4 Feb 13
: **Lec**{: .label .label-green }Self-Supervised Representation Learning and Object Discovery
  **Tut**{: .label .label-purple }[Egocentric Video Tutorial](https://embodied-learning-vision-course.github.io/course-public/2025-spring/lab/lab4_ego4d.pdf)
: - Suggested readings:
    - Sermanet et al. (2017) [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
    - Van den Oord et al. (2018) [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)
    - Wu et al. (2018) [Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination](https://arxiv.org/abs/1805.01978)
    - Chen et al. (2020) [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
    - Grill et al. (2020) [Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)
    - He et al. (2021) [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
  - Additional readings:
    - Weinzaepfel et al. (2022) [CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow](https://arxiv.org/abs/2211.10408)
    - Wang et al. (2022) [Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut](https://arxiv.org/abs/2202.11539)
    - Seo et al. (2022) [Masked World Models for Visual Control](https://arxiv.org/abs/2206.14244)
    - Venkataramanan et al. (2023) [Is ImageNet Worth 1 Video? Learning Strong Image Encoders from 1 Long Unlabelled Video](https://arxiv.org/abs/2310.08584)
    - van Steenkiste et al. (2024) [Moving Off-the-Grid: Scene-Grounded Video Representations](https://arxiv.org/abs/2411.05927)
    - Cui et al. (2024) [DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control](https://arxiv.org/abs/2409.12192)
    - Wang et al. (2024) [PooDLe: Pooled and Dense Self-Supervised Learning from Naturalistic Videos](https://arxiv.org/abs/2408.11208)

Week 5 Feb 20
: **Lec**{: .label .label-green }World Models and Forecasting
  **Tut**{: .label .label-purple }Motion Learning Tutorial
: - Suggested readings:
    - Kalchbrenner et al. (2016) [Video Pixel Networks](https://arxiv.org/abs/1610.00527)
    - Ha and Schmidhuber (2018) [World Models](https://arxiv.org/abs/1803.10122)
    - Hafner et al. (2019) [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)
  - Additional readings:
    - Liang et al. (2020) [Learning Lane Graph Representations for Motion Forecasting](https://arxiv.org/abs/2007.13732)
    - Wu et al. (2022) [DayDreamer: World Models for Physical Robot Learning](https://arxiv.org/abs/2206.14176)
    - Yu et al. (2022) [MAGVIT: Masked Generative Video Transformer](https://arxiv.org/abs/2212.05199)
    - Hafner et al. (2023) [Mastering Diverse Domains through World Models](https://arxiv.org/abs/2301.04104)
    - Hansen et al. (2023) [TD-MPC2: Scalable, Robust World Models for Continuous Control](https://arxiv.org/abs/2310.16828)
    - Zhang et al. (2024) [Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion](https://arxiv.org/abs/2311.01017)
    - Casas et al. (2024) [DeTra: A Unified Model for Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2406.04426)
    - Bruce et al. (2024) [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
